- consciousness vs materialism arguments
- does not having consciousness actually prevent discovery? can rules produce discovery? perhaps, but can they produce insight? could they derive things that aren't even in the training set e.g. information theory?
- what is intelligence?
- can intelligence and/or consciousness emerge from mechanistic processes?
- what would symbolic AI be classified as?
- I don't find all of these to be very convincing, but they represent a common dualist point of view. There are some interesting comments:
  - https://www.youtube.com/watch?v=E31KuUJmqCU ( AI is a Nothingburger. You're wrong. )
  - https://www.youtube.com/watch?v=V7abyVkS6U8 ( Consciousness Is Not Material. )
  - https://www.youtube.com/watch?v=gleyKDvZ3x0 ( Computation isn't Consciousness: The Chinese Room Experiment )
  - https://lukesmith.info/articles/consciousness-and-materialism/
  - https://lukesmith.info/articles/searle-dennett/

> I completely agree with you. Sadly the people who don't understand this point will still misconstrue the Chinese room experiment. When we talk about computers "learning"" or "thinking", it's very  different than human learning and human thinking, because we have consciousness, first-person subjective experience (which is not material/physical).  Another example i give is a person who is born blind, such a person "knows" colors" but doesn't really know them the way a person with sight understands color.
>
> Hardcore materialists like Daniel Dennett can't make sense of consciousness because it doesn't make sense based on their worldview, that's why some suggest consciousness is just an illusion.

> If consciousness is made up of something beyond the physical, then we must answer the question: at what point in evolutionary history was this non-physical element embedded in a byproduct of a material mutation?
> Given that the non-material cannot result only from something purely material - otherwise, it would merely be an extension of the material world.

> does gravity stop existing as an intrinsic property of matter (we could say as a building block in phenomenological reality) when it is no longer detectable at a certain scale?
> Consciousness was always there, we just became able to receive it. I agree that "when?" is a cool question to ask.

> Or better yet: Where does math come from? It is not material, yet it can be used to measure and understand the material world in some way. At this point, one must use metaphysical instruments and assume that there is a metaphysical reality.

> To me it's entirely plausible that the physidal universe emerges from consciousness but I cannot conceive of how consciousness could emerge from a physical universe.

> All the chinese room experiment 'proves' is that individual neurons (the people in the experiment) aren't conscious. It can't speak at all to what the aggregate effect of information processing is.

> "No, we're just single ceels. We exchange information, but the brain isn't really conscious. It might seem like consciousness, but that's just the result of us interacting, nothing more."
>
> > People will believe rocks and neurons are conscious before they admit that maybe there is something missing in materialism.

>  I agree that current state "AI" is not conscious, it's just a parrot. However, I think there's still room for the theory that consciousness arises by a specific set of principles or processing capabilities being in place, including, but not limited to, self awareness and regulation; awareness, regulation of and interaction with the environment, and self preservation drive. When those conditions are met, even if it may seem "artificial", "fabricated" or "forced", it would be hard to argue the system is not conscious, by using a Markov blanket to clearly define the system's boundaries. In that sense, countries and even cells, in my opinion (probably an unpopular one) for example would be conscious systems themselves.
>
> >  No, this is mistaking consciousness for consciousness-reminiscient computation. Consciousness might have, for example, recursive characteristics, but that doesn't mean that it is a recursive neuronal operation, or that a computer program with recursion in conscious.
> >
> > To be clear, it might interact with recursive systems, but this is apart from consciousness, not as a description of behavior, but as qualia and experience in itself. So we can make an AI or a recusive system which acts like a conscious being (for the most part), but that computation does not make it conscious.
> >
> > Or to put in a simpler way, philosophical zombies exist.

> I think you can't prove that other people are conscious precisely because of these issues as well. So how could we possibly prove that an AI is conscious even if it was?
>
> > People needs to stop with their magical beliefs and start thinking. The they'll see that consciousness is nothing special, but a simple byproduct of processing complexity.
> > If we have it, future machines will have it.

> 1. it is not the individual human in the room who should be considered the locus of understanding, but rather the entire system as a whole. Just as understanding in humans doesn't reside in any single neuron but emerges from the entire neural system, understanding Chinese might emerge from the whole room system - including the rulebook, the paper, and the person functioning as a central processing unit. The person is merely one component in a larger cognitive system that, in its totality, understands Chinese.
>
> 2. understanding might exist as a "virtual" entity within the system, similar to how virtual machines and objects exist in computing. When we call up the pocket calculator function on a desktop computer, the image of a pocket calculator appears on the screen. We don't complain that it isn't really a calculator, because the physical attributes of the device do not matter. Similarly, understanding might be instantiated virtually within the Chinese Room system even if no physical component individually possesses understanding.
>
> 3.  if the Chinese Room were embedded in a robot that could interact with the world - receiving sensory input and producing motor outputs - then the system would have the necessary causal connections to develop genuine understanding. By interacting with the environment, the symbols would be grounded in real-world referents, creating the connection between syntax and semantics that Luke claims is missing. If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world
>
> 4. Consider replacing neurons one by one with artificial components. At what point would consciousness disappear? Critics argue there would be no clear line, challenging the video's sharp distinction between computation and consciousness
>
> 5. Luke is trying to get you to place the mind of the room in the man, and because the mind cannot be said to be in the man, the room has no mind. However, the mind isn't in the man, it is the room, and more specifically, the book in the room
>
> > The problem is the premise that a model of a system that shows emergent properties is the same as the system itself. If you had a complete description of all the states of the universe in one sheet of paper and did all the computations one at a time and then copy the new states to a new piece of paper, are the mental states described in the paper or at the moment of being calculated conscious? If it was like that running the simulation of a brain would have moral implications. I don't believe there is something "spiritual" in the matter, but reality is not mere computation.

> I don't get the cope mindset that "consciousness must be different than everything else we observe".
> Complex systems emerge from less complicated ones all the time, or 2D things do become 3D (phrasing from your argument).
> Accepting that I am nothing but a bunch of interacting bits of fundamental fields doesn't make me nihilistic, if anything, I appreciate life even more.

> Dualism is just materialism with one extra step, and all of the same inherent contradictions still apply. Idealism is the only internally consistent metaphysical worldview and we've known this for thousands of years.

> There is a good amount of wisdom in hating thought experiments.
> The chinese room experiment is demonstrating all the flaws of thought experiments creating over-simplified models of something then deriving wrong results from wrong reasoning on the wrong models.
> Chinese room is unfeasible unless it has state, it is impossible to do it with a lookup. It must be at least a turing machine to compute new states and have a vast algorithm for that. Now, it is hard to argue that intelligence cannot be computed on a turing machine in one way or another. Now, consciousness is a special process which emerges when an intelligence can perceive itself.
> I would not go further into this, but in correct intrrpretation the Chinese room is potentially conscious, without any regard to the consciousness of its inner operator.

> "syntax isn't semantics" -> "consciousness isn't computation" -> "minds aren't physical". These are big jumps.
> Searle was against computationalism, not physicalism: aka the mind is physical, but not purely computational.
>
> Manipulating discrete symbols using explicit rules is one of an infinite number of "machines" you can create, and Searle believed there was some other type of machinery (more complicated than if-statements) that could explain consciousness. For example, this machinery could be chemical instead of simply programmatic/symbolic. Our brain can "compute" things using neural networks, sure, but there are a wide array neurotransmitters which interact with this system. These could be the "substance" of consciousness (or any number of other mechanisms in the brain). People would just always say they feel consciousness is too big to boil down to some chemicals though. It's also worth noting that this system (and neural networks in general) is analog and not digital. Anyway, I don't think this thought experiment really says anything about whether these other types of machines could be conscious (or contribute to consciousness). It's just about how one type of simple machine probably isn't conscious.
>
> To get a better feel for how people think about this: Would you consider a clone of yourself to be conscious?
> I feel it's too easy for people to say a perfect, atomic simulation of their brain wouldn't be conscious, but a biological clone hits a little closer to home on the dualism issue I feel.

- Aristotlean vs Platonic view of knowledge and what is knowable, does the material world matter?
- None of these ideas or arguments are new, but what do they mean for AI?
  - https://en.wikipedia.org/wiki/Idealism (mental monism)
  - https://en.wikipedia.org/wiki/Materialism (physical monism)
  - https://en.wikipedia.org/wiki/Mind%E2%80%93body_dualism (dualism)
- https://news.ycombinator.com/item?id=44089156 (Chomsky on what ChatGPT is good for (2023) (chomsky.info))
