# Will We Hit a Dead End? Will AI Saturate?

- See the AI 2027 article. (https://ai-2027.com/)
  - The predictions for the future all hinge on one thing
  - **Self improvement**, research into itself that can feed back and continue indefinetly, automatic discovery
  - Without self improvement, human injection / steering is always required and although the rate of "progress" may improve for mechanistic "research", truly revolutionary or innovative or creative endeavors will be bottlenecked in the same way
- The symbolic AI lesson
  - Cyc, Eurisko, Automated Mathematician, they all saturated
  - https://en.wikipedia.org/wiki/Discovery_system_(artificial_intelligence)
  - Heuristics, meta-heuristics, and Cyc itself - after some point, there was nothing more it could do
  - Saturation seems to be a hard thing to break out of - human intervention was always needed
  - Is 'common sense' (grounding knowledge) really necessary for continuous improvement and discovery? It seems that basis of Cyc is iffy.
  - Do we understand theoretically why Eurisko saturated? Why it couldn't produce new interesting heuristics? It seems unclear. Even with brute force search, it came up empty? Or rather that brute force search would be computationally impractical even for today's computers? Is intelligence just a matter of search over meta-heuristics and grounding or is there more?
- Will LLMs saturate?
  - It certainly seems so. They can do lots of things, but they are still limited by their training data (heuristics and meta-heuristics and common sense grounding / knowledge)
  - "Agentic" research AIs (see Sakana ai or Google's AI scientist) have a hard time producing creative output. They are very good at 'mechanical' research, but so are humans (especially the Chinese).
    - https://sakana.ai/ai-scientist-first-publication/
    - https://github.com/SakanaAI/AI-Scientist-v2
  - The poor performance of these same models on things like KernelBench indicates that they can't self-improve meaningfully (yet)
  - There is some *magic* that we don't yet understand which would explain why these models are so mechanistic and unable to be creative even with millions of instances running (and same with symbolic AI). AI optimists hypothesize that there is no magic and enough training data and enough instances is sufficient to break through the nominal saturation point.
  - Can we combine the grounding of LLMs with symbolic AI? Can this grounding alone generate symbolic facts and heuristics that can break out of saturation? I doubt it.
  - Symbolic "AI scientists" have produced interesting outcomes in the past (https://en.wikipedia.org/wiki/Glauber_discovery_system) just from search and heuristics alone.
- What is the role of randomness?
  - If it is was the case that randomness was sufficient to induce creativity then we are done. We can just play with many LLM instances and temperature.
  - There is some relationship between entropy, creativity, and reliability / reasoning, but I'm not sure how to think about this. Is the 'randomness' inherent in the physical world (quantum randomness) something that is necessary for creativity to emerge? Why isn't psuedorandomness sufficient? Is there a non-religious explanation for this?
- Can reasoning exist independently of language? Can it exist independently of knowledge / facts / physical reality?
  - Most would answer yes. Aristotle may say no.
  - But can language alone induce reasoning? Now the symbolic AI people would say no and the language modelers would say maybe yes. They hypothesize: next word prediction with planning = emergent reasoning with enough grounding.
- Is knowledge and reasoning enough? Having infinite knowledge and sufficient 'reasoning' hasn't produced much of anything.
  - https://www.dwarkesh.com/p/scott-daniel

> Well, I asked this question where, as you say, they know all this stuff. I don’t know if you saw this. I asked this question where I said, look, these models know all this stuff. And if a human knew every single thing a human has ever written down on the Internet, they’d be able to make all these interesting connections between different ideas and maybe even find medical cures or scientific discoveries as a result.
>
> There was some guy who noticed that magnesium deficiency causes something in the brain that is similar to what happens when you get a migraine. And so he just said: give you magnesium supplements that cured a lot of migraines. So why aren’t able to leverage this enormous asymmetric advantage they have to make a single new discovery like this?

Dwarkesh asked this question and I think Scott and Daniel failed to produce a good counterargument. I recommend you read this back and forth in the transcript.

- How does this relate to our environments?
  - Cities seem ideal for innovation right? Lots of smart people next to each other. Lots of universities, constant action, easy access to manufacturing. Certainly this is the case in China.
  - But where have we seen the largest innovation and insight historically?
    - Ancient world: sparsely populated city-states (e.g. Athens)
    - Industrial revolution, Renaissance: sparsely populated cities OR urban areas where the top thinkers had plenty of solitude, isolation, and a non-dense work environment (think monasteries)
    - Modern day: Palo Alto, Cupertino, Mountain View LOL (notably not New York or Philadelphia or London, but rather the *suburbs* of those areas)
    - What is the common theme? Few people produce the greatest innovations. Creativity is suppressed not because of "routine" but because of "density" (at least this is my hypothesis).
    - More people = more research output? Very questionable. The basis of lots of agentic AI hypotheses is that more AIs = more progress.
    - How does 'density' of human habitation relate to the 'density' of LLMs (not in a physical way of course)?
