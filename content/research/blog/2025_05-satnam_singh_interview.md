+++
title = "Commentary on an Interview of Satnam Singh"
date = 2023-08-23
draft = true
description = "Groq, HDLs, PL for HW design"
+++

- https://www.youtube.com/watch?v=_g5fA9Cy6KU

> Current Continuation is our little interview series where we talk to researchers in the field of programming languages. In this episode, we talk to Dr. Satnam Singh who is a scientist at Groq https://satnam.fpcastle.com/

Satnam is one of the smartest people I've briefly spoken to and I found many interesting tidbits in this interview.

- Interesting things
  - https://en.wikipedia.org/wiki/Miranda_(programming_language)
  - The difficulty of getting Lava or other eDSLs for HW design proliferated in a semiconductor design team
  - Groq uses an exact cycle-by-cycle scheduling scheme, which implies that even cross-chip and cross-blade communication is cycle-precise, just like a Palladium emulator

> I'm not sure that the best way to design a chip is to hire people who are normal chip designers who know SystemVerilog. I think the best way to make a chip is to hire the best PL people you know and retrain them how to do digital design. We did in the first chip company I was working at; we actually did quite a bit of that.

I tend to agree, but implementing this in practice when you have VCs to answer to and a strict timetable is easier said than done. In principle, Google would have been an ideal place to trial this, but the Edge TPU Chisel experiment failed in the long run.

## Transcript from Gemini

- This is quite cool
- `yt-dlp --skip-download --write-subs --write-auto-subs --sub-lang en --sub-format ttml --output "transcript.%(ext)s" "https://www.youtube.com/watch?v=_g5fA9Cy6KU"`
- Rename the file to `transcript.txt`
- Upload to Gemini 2.5 Pro on Google AI Studio and give the prompt:

> This is a transcript for a video with the title "Satnam Singh (Groq)" and description:
>
> (insert description from the Youtube video)
>
> Please format this transcript with speakers (Adrian, Satnam, Aws) and format it into sections based on content. Make sure you preserve the transcript in its entirety, but feel free to remove um/uh/filler words and construct clean sentences.

- And that's it! A perfect transcript is produced. This really feels magical and makes podcasts actually useful. Spending hours listening to podcasts is unacceptable, but skimming a perfectly formatted and sectioned transcript in a few minutes is doable.

**Title: Satnam Singh (Groq)**

**Description:**
Current Continuation is our little interview series where we talk to researchers in the field of programming languages. In this episode, we talk to Dr. Satnam Singh who is a scientist at Groq https://satnam.fpcastle.com/

Zoom failed us and Adrian's camera wasn't captured in the recording.

Here are links to the people and projects mentioned:

*   Satnam Singh - Personal website: https://raintown.org/satnam/
*   Philip Wadler - https://homepages.inf.ed.ac.uk/wadler/
*   Mary Sheeran - https://www.cse.chalmers.se/~ms/
*   Groq - https://groq.com/
*   Xilinx - Now part of AMD: https://www.xilinx.com/
*   Lava (Functional HDL) - Satnam's project: https://raintown.org/lava/
*   Lava GitHub Repository: https://github.com/satnam6502/lava
*   Brett Restaurant Glasgow: https://www.brettrestaurant.co.uk/
*   Octomore Whisky from Bruichladdich distillery: https://www.bruichladdich.com/
*   FP Castle - https://fpcastle.com/
*   BBC Micro - https://en.wikipedia.org/wiki/BBC_Micro

---

**Transcript**

**[00:00:00] Introduction**

**Aws:** Alright, hello everybody. You're on a call with Current Continuation, which is a podcast. This is the second episode. We're really happy to introduce Satnam Singh. He is currently a fellow at Groq. Groq is an AI chip company that deploys LLMs at scale, and it's really, really fast. At least that's what they say on Twitter; I've never used it. Before that, he worked at Google, Facebook, and Microsoft. A long time ago, he was also an academic and received his PhD from Glasgow. He works on really interesting stuff at the intersection of hardware and programming languages. So, we're really excited to have this conversation with Satnam. Thanks for joining us today, Satnam.

**Satnam:** Pleasure to be here. Thank you for having me.

**[00:00:53] Satnam's Early Life and Getting into Computing**

**Aws:** So, maybe we should start at the beginning. You've had an interesting upbringing. Tell us about your life.

**Satnam:** Well, my life started in India, in Punjab, Northwest India. When I was one and a half, my mother and I moved to Glasgow in Scotland. So, I grew up in Scotland, even though I was born in India. My father had gone ahead to pave the way for us. I grew up in a Sikh, Punjabi-speaking family and turned up at elementary school, age five, not speaking any English. I was put on some special track for a couple of years to learn English before I was reintroduced to the normal track. My parents only spoke Punjabi and didn't speak any English. In fact, they couldn't even read or write Punjabi because they were both children and didn't have any education. So, it's a bit odd because in my family, I was the first person that went to elementary school, high school, undergraduate, PhD, and became a lecturer, or professor as you Americans call it, etc. So, that's where I kind of come from.

I got into computers in high school. I was quite interested in physics, and the physics teacher, Mr. McClure, really took me under his wing. He made me join the chess team because I was reasonably good at chess, so I played in chess competitions. After school, he had a computer club. This was in the late '70s, early '80s – [Correction: The transcript says late '7s early early 90s which is likely a typo for late 70s/early 80s given the computers mentioned, or late 80s/early 90s. Given ZX81/Spectrum/BBC Micro, late 70s/early 80s is more accurate for their peak popularity in schools. However, his PhD was in 87-91, so high school would be early-mid 80s. Let's assume late 80s/early 90s as stated, which means these were older machines still in use or he misspoke the decade slightly.] – and the kind of computers we had were things like Sinclair ZX80, ZX Spectrum computer, and a BBC Microcomputer, a 6502-based computer. I just fell in love with these computers. They seemed like magic. You typed in some BASIC, you ran it, and things came up on the screen. What captivated me was I just couldn't understand how this could work. How is it you type these things, you run it, and some computation happened or some string got written?

I persuaded my parents to try and get me a computer, which was tough because I came from a pretty poor family. My father was a construction worker, then he ended up on welfare, so we didn't have a lot of money. What I did was I convinced my parents to spend the money the government would give them to buy me a school uniform. Although I went to a state school, it still had a school uniform requirement. So, instead of using that money to buy a school uniform, I said, "Buy me a BBC Micro Model B computer." We got a free school uniform from the local charity shop. I just spent my teenage years writing BBC BASIC, 6502 assembly, BCPL, trying to implement mini compilers, mini operating systems. That's how I ended up in the world of computing.

As an immigrant in the UK, my only path forward was education. I wasn't going to work in my auntie's insurance company or my uncle's secondhand car sales company or whatever, because we didn't have any connections to the community. What's good about the UK was it had grants to support kids from poorer families. So, I didn't have to pay anything for my university education or my books. I got some loans. Both my undergraduate and my PhD were paid for by the state, which was an amazing thing about the UK government at that time, because I don't think I would have gone to university otherwise. So, that's how I ended up in computing and education from a background of immigration.

**[00:04:47] The BBC Micro and Early Hardware Interaction**

**Adrian:** Satnam, do you mind if I ask, just because you mentioned in passing programming in 6502 assembly, which of course totally makes sense. How did you find the book? Did you check it out from the library?

**Satnam:** I wish I had it with me as a prop. The BBC Microcomputer comes with a user guide, the BBC user guide. It has everything about BBC BASIC, it has the schematics of the board, every chip, every detail. BBC BASIC has an inline assembler. So, you write BASIC, and then you drop into 6502, and then you can use some of the variables from the enclosing scope from the BASIC. Then you could do a multi-pass compilation by having a for loop where you compile several times to resolve the forward and backward symbols, etc. So, I learned it from the book. I did then buy more; there are some official BBC 6502 programming books, and I bought them to learn how to do more 6502 programming. But even from the BBC BASIC user guide, you can learn quite a lot.

**Adrian:** It's amazing what's with manuals these days.

**Satnam:** Yeah, I love the fact that this computer came with a user guide which told you everything about the computer: operating system, memory map. Where do you want to write an 'A' on the screen? How do you do it? Well, LDA 65 (that's capital A), JSR FFFF (jump to that point), and then an 'A' will get rendered on your screen. Every kind of detail was described in this book. Or if you want to create a special bit of circuitry to do some music synthesis, whatever, you create a circuit board and you connect it to the special serial parallel port under the machine, and it has the spec of all the wires and the voltages and the protocol, and you could code it up. That was such an amazing thing.

One thing I love is breaking the distinction between hardware and software; for me, it's just a system. It's the end system. This user guide, I think, had that kind of philosophy: you have this box in front of you that computes, here are the things you need to know about how to make it compute. And those things, they cross hardware and software: programming 6502, how to program the Hitachi 6845 video controller, how to program the Teletext chip. It was all described and all available to you. I miss this intimate connection with the hardware that I had in the BBC Micro because today we have so many levels of virtualization and abstraction between the program we write and the reality of how it executes, and the reality of which electrons go at which transistor junction. All these pesky operating systems and virtual memory and all this really get in the way.

**Adrian:** And hypervisors and all kinds of things like that.

**Satnam:** And even the 6502 was a very simple processor. When you wrote your code, you knew how many cycles it would take to execute, and you had some feeling about the economics of that execution. Today, we have these ISAs for x86. Our compilers generate these x86 instructions. Those instructions are very, very, very faintly related to the actual instructions that get executed in your processor core. And how many cycles would it take to execute? Well, you have no idea about that. That's a very complicated, sophisticated thing. So, an ISA is now like a contract between the world of compilers and the software stack, and the people that implement hardware. A contract abstraction which I think is under stress as we now have these kind of multi-core processors and heterogeneous computing, and we're being forced to get performance from paths other than the scaling expected through Moore's Law. The world of heterogeneous computing is stressing that ISA abstraction.

**[00:08:50] Path to Grad School and PhD Work**

**Aws:** Totally. So, if you don't mind returning to this sort of chronological view, we were wondering also: you perfectly described how you got into computing in the first place and into programming. How did you decide that grad school was for you? That's not necessarily a given.

**Satnam:** So, I went to university, and I went to study physics because I was very interested in physics. I thought I wanted to be a physicist. I wanted to do nuclear physics. I wanted to work at the Deutsches Elektronen-Synchrotron in Hamburg. Not only did I study physics in high school, I studied German because I thought I was going to end up in Hamburg. I did very well at physics at my high school, but just like everyone else in my physics class in the Kelvin lecture theater at University of Glasgow, where Lord Kelvin used to give his physics lectures, well, everyone else was top of their class as well at high school. And now I'm realizing, "Hmm, I am not necessarily going to be the top of this class." I did fine in physics; I got pretty good scores, but I had to work quite hard on it.

I had to pick a third subject, and I picked computing, not expecting to graduate from it, just as an elective. What I observed was I got nearly 100% in computing and didn't require much effort. The physics, oh, that was hard. So, then I switched my degree in my second year to do a joint degree between electrical engineering and computer science. So, I changed paths.

But I always thought I wanted to do a PhD because at school, I worked out how to become a nuclear physicist. I sent letters (back then there was no email), and the physicists that would reply to me would say things like, "Well, Satnam, if you want to become a physicist, the first thing you want to do is get a PhD in physics, and before that, you need to get a degree." So, even at high school, I kind of knew what a PhD was before I knew what a degree was. I kind of knew that was my path. I was going to do an undergraduate degree, and then I was going to do a PhD.

When I went to university, I was actually really inspired by some of my professors. Some were just so wonderful at teaching, had such passion for the subject. I remember specific people like David Watt's lectures on Ada and things like that; they were wonderful. I thought, "That's a very cool job. That's a very cool person. I want to be like that." I started to question whether I wanted to be a nuclear physicist. By that time, I think I'd already decided I was doing computing. So, I fell in love with the idea of being an academic, teaching in a collegiate environment, thinking deeply about problems. I thought, "Yeah, that's probably for me." So, I was inspired by my professors.

**Aws:** And what was your thesis work on at Glasgow?

**Satnam:** So, my final year undergraduate project was about creating a functional language to program these special kinds of chips called PALs and PLAs. There are chips which you could configure; you could download a bitstream, you could configure them to make some computation. They have some general assembly of either a sum of products or product of sums, and some inverters you could put in various places. I don't know why I thought it was a good idea, but I decided to create a functional language. I created my own project.

**Aws:** So, you went from coding in assembly all the way to functional. What was that like?

**Satnam:** To me, I just view it as a progression of the same thing. I thought it was necessary because I think what's missing in the world is a diagonal. In the world, we've got high-level things for solving high-level problems (Java, web interfaces) and low-level things for tackling low-level problems (assembly code, C for low-level systems). But I think a wonderful thing to have are high-level things to make you productive for doing low-level tasks. That's what I wanted. I wanted to supercharge and make myself more productive at doing all this low-level stuff, not give up control. I still want to control the hardware, its economics, and its performance. I just want to be more productive at doing that.

I felt that there was some natural synergy between functions and how circuits work; they just seemed well-matched to me. So, I kind of hacked up this language. I mean, I knew nothing really about languages; I was a final year student. But I had this language, I remember I called it Pineapple for some reason, and wrote a wee compiler for it. So, you could write these recursive functions and describe smallish to medium-sized logic blocks. I would analyze it, and then it would generate the program bitstream to program these PAL and PLA chips. I kind of loved that, and I think the project went very well.

I knew I wanted to do a PhD, and I applied to several places. I got rejected from most of them, but I got two offers. One was at University of Edinburgh with Robin Milner, and the other was staying at Glasgow with Mary Sheeran, who had been my final year computer architecture professor. One reason I went to University of Glasgow is I actually grew up pretty much on campus. My high school was on the university campus. When I sat in my high school, I could see the university tower outside the building. The electrical engineering department is on the same street, like two blocks down. So, I had a small view of the world, probably because of my Punjabi immigrant child upbringing. I just assumed that you just went to a university that happened to be on your school's campus. So, I just applied to Glasgow; I don't think I applied anywhere else for undergraduate education.

Then when it came to the PhD, I was offered these two options. Edinburgh, 40 miles away, seemed like a vast distance. I could not imagine going that far away from home. I stayed at home when I was an undergraduate; I lived with my parents. It was just like a 10-minute walk to university, which was the same place my school was. So, I thought about it. The other thing about Edinburgh: Professor Milner and Professor Burstall seemed like amazing people. They had an idea for a project for me, which was to work on a parallelizing compiler for Standard ML. Man, that sounded pretty interesting. But he showed me where I would sit if I'd be a PhD student, and this thing, it wasn't even really an office; it was more like a cupboard, and it already had two PhD students in it, and I would be sort of crammed in between them somewhere in this kind of gray monolith in the south of Edinburgh. Whereas at Glasgow, I was in these beautifully converted Victorian buildings with stained glass windows, with spacious rooms for the PhD students, and it's 10 minutes from my parents' house. So, I still stayed with my parents when I started my PhD.

I decided to do a PhD with Mary Sheeran: more fun with functional programming and hardware design. I was quite inspired by Mary's final year lectures on computer architecture, but described in a kind of more formal, symbolic, functional way. I remember her lectures about transputers and Occam. I just loved all this synergy between a way of computing, some architecture, and a way of modeling and programming, and when you seem to get a really good impedance match between the two things. I was captivated by all these things that Mary lectured about. So, I signed up with Mary instead, and I was her first PhD student. I think my life would have been quite different had I done a PhD with Robin Milner. I often wonder what that would be like, but I'm very happy with the choice that I made and the path I took.

**[00:17:01] Hardware Description Languages: Miranda, Haskell, and Lava**

**Aws:** So, did Lava come from the PhD work?

**Satnam:** No, not at all. Lava came a lot later; it came from when I worked at Xilinx. In my PhD, I did work on functional industry model hardware. At that time, Miranda was around; GHC had not started. I started my undergraduate in 1983 for four years, and then I started my PhD in 1987, so it was pre-GHC. I used Miranda, and I absolutely loved the fact that Miranda was an even better match for modeling hardware than this Pineapple thing that I had done. The reason for that was lazy evaluation. I could directly describe a circuit in the language, run it, and give it some values, which I couldn't really do with other languages. This is to do with state and loops. In Miranda, you could describe a state element simply by writing a function that took a list as its input, and at the output, you add an element to the front of the list (undefined, whatever, zero) and you `cons` the list you got as input. That has delayed that list one step in time if you consider each element of the list as being a value in time. What a beautiful way of describing what delay means.

Then in sequential circuits, you often want to have a value fed back, like a counter. So, you take the previous value, you add one to it to get a new value. Well, you just write that directly. You can have the value `x` on both sides of the equals, and because of lazy evaluation, the right thing happens. You give it a list of some finite size, and then you just take the finite size list at the output, and you get the answer. So, you have this direct, kind of almost isomorphic representation of this physical thing in the world with a very light and beautiful, small set of abstractions. That just seemed wonderfully compelling and kind of magical to me. I really loved that. People often struggle to justify lazy evaluation because it comes with so many costs and complexities and difficulty for analysis. But when it comes to modeling things like state and feedback in digital circuits, I think it's perfect.

**Aws:** And I guess that's your connection to Haskell, that's where perhaps it started?

**Satnam:** Well, yeah. Towards the end of my PhD, I think Haskell was coming along. The kind of lazy evaluation and the kind of list processing functions that you write in Haskell are very similar because Haskell is like an evolution of Miranda. Through my PhD, I built up this system where you could describe circuits using Miranda. I had multiple interpretations for circuits, like think of them being overloaded. This was before Miranda had type classes; they were being developed by my fellow PhD student, Stephen Blott. Stephen and I were undergraduates together at Glasgow too. Stephen worked for Wadler to do type classes, which would then find their way into Haskell.

I came up with some system which let me describe a circuit graph and have one interpretation where you simulate it, another interpretation where you could compute the critical path (the delay through it), another interpretation where it could draw itself, another interpretation where it could compute testability vectors by composing other non-standard interpretations. That was the topic of my thesis: non-standard interpretations for circuits. Capturing what's common across lots of different computations, abstracting that, and then instantiating it with the differences, and then composing them. That was inspired by the work I'd done the summer before at a silicon chip company called European Silicon Structures, where I was like an intern for three months. I had to manually write these programs again and again. I felt like I'd seen this before; this is the same pattern. Why am I rewriting this C++ again and again? I felt that this is something that should be abstracted. So, looking for these things that get repeated as patterns and turning them into useful, common use abstractions, and then composing them hierarchically, that was the inspiration that kind of drove my thesis work.

**[00:21:19] The State of Hardware Design Languages**

**Aws:** That's very interesting. I'm very far from hardware, so I don't have a good view of what hardware design languages are like. I mean, I've used Verilog in undergrad, and that's where I stopped. You're describing these very sophisticated, interesting languages from the early '90s, late '80s. What has happened since then?

**Satnam:** I feel that there's just a tragedy in the world of hardware. The mainstream hardware languages, honestly, I just don't think they are fit for purpose. They have very troublesome underlying semantics, and they were never designed to make hardware; they were designed to model hardware. Verilog is the most commonly used language. I used VHDL a lot because I was in Europe, and VHDL was much bigger in Europe, but they have similar kinds of semantics. Their semantics are based on event-based simulation because these languages were designed to simulate circuits, not to make circuits or create them. They have event-based simulation semantics, which is completely sequential. This is absurd. You have a totally sequential model to try and simulate something that's inherently parallel, where every gate is running at the same time as every other gate. At the same time, computer scientists are developing multi-threading and concurrent programming for programs that run on one core. So, their model is parallel/concurrent, and it runs something that's inherently sequential, and hardware people run it the other way around. This just seems totally mad to me.

I don't think a huge amount has really changed. Verilog has morphed into SystemVerilog, but at its heart, it still has this troublesome event-based simulation semantics. People have tried to make other languages for hardware design. We have tried; I made my DSL Lava at Xilinx, but it still works by compiling to Verilog. There's been great work done by the people at the company BlueSpec that made this fantastic Haskell-inspired hardware description language, which I used for one of the chips that I worked on at Google. It's got very interesting, sophisticated types, lots of powerful abstractions, but it still works by generating Verilog. So, the reality today is if you want to actually make a chip or design, you can create all these terms of abstraction, but the standard kind of API down to the gates is Verilog with its troublesome semantics and tooling and kind of sequential model of the world.

During my PhD, I didn't actually get them mapped into real hardware. I produced this system that would compile to VHDL, and you could run simulators. Then I was lucky to get a position as a lecturer in the electrical engineering department at the same university. So, after seven years of study, I literally moved two blocks down the same street in Oakfield Avenue, because it was just adjacent to my high school. I sort of settled into my office, and then the guy who was my manager at this ES2 company where I worked as an intern before my PhD, John Gray (he was a professor at Edinburgh), had started a new company in Edinburgh. He created these new FPGA chips he was excited about. He comes and visits me, kicks my door in, and brings me this FPGA board. It's a PCI card – some of you might never have seen an actual PCI card, no 'e' in its name. It has these very interesting FPGAs; you can download logic onto these kind of Lego bricks of hardware, and it will do anything. You plug it in, and it's into your memory space as a configuration state. So, you can write a C program, and just by reading and writing to memory locations, you could configure computations on this chip, execute them, and run them. They'd made this amazing board. He said, "Satnam, I saw your PhD thesis. Interesting work, but come on, man, do something real. Here's a board. Work out how to program it. I'll be back in three months. I expect to have a working compiler." He storms off back to Edinburgh.

So, here I am, given this card and this challenge because he's like a real electrical engineer, and he thinks all this kind of Miranda/Haskell stuff is kind of fine, but he wants to see something real done with it. So, I tried to start to develop tooling and applications which exploit these chips and the fact you could change the hardware as you run. This then led to making programming abstractions for reconfigurable hardware. Imagine chips where you're swapping bits of hardware in and out, like virtual hardware, like virtual memory, except that you're doing it with hardware blocks. I built a wee bit of a research career for myself doing that, mainly publishing in FPGA conferences, but trying to take ideas from the worlds of programming and operating systems and virtual memory, but reimagining them in a world where you could change the hardware as it's running. The hardware itself could compute the next bit of hardware it is. That was all kind of mind-bending stuff, and quite a lot of the '90s FPGA research was about how to try and harness the power of this dynamic reconfiguration because now you can have a virtual circuit that's much bigger than the physical limitation of the chip. That was another great forward step, and I kind of owe John Gray a lot of gratitude for giving me a kick in the right direction.

**[00:27:01] Lava, Physical Layout, and Solving Real-World Problems**

**Adrian:** So, about that work, about transitioning to the FPGA world and Lava eventually. There are a lot of things that are really inspiring about Lava; it's a huge inspiration for many of us working on hardware design languages today. Something that I feel like is maybe underappreciated about it, or maybe in contrast, how few languages today have anything to say about the physical layout of circuits. There's a lot of really exciting work going on in hardware description languages and high-level languages for generating hardware, but very little of it has anything in the semantics about the physical instantiation. We're just sort of content living in this world of a graph of the interconnections of components or something. I'm just curious what your thoughts on this are, in particular, what the modern world should learn from this type of approach.

**Satnam:** A wee bit how we got there. I started off with a pre-version of Lava for programming this chip, and these chips are pretty small; they don't have that many computing elements. So, you have to be very careful and frugal about how you lay your circuit out, and whether this thing is close to other things. You really want to control your critical path. So, out of frugality and necessity, I had to come up with a model which was really aware of location.

At Xilinx, the reason Lava got made was because we had customers who could not fit their designs into the FPGA chips. They picked a particular chip for a particular size (that's what they could afford), and they had some design, an FFT for a software-defined radio, and they couldn't quite jam it in. The field application engineers from Xilinx were helping the customers; they just couldn't get the tools, the place and route tools, to work. They take an abstract circuit netlist, they roll dice a lot, and they try, through simulated annealing, heating, cooling, to see if it would go in, and they just couldn't get it to work. It's only when people are desperate do they go to someone else and say, "Right, you've got this weird other stuff, can you help me? Because we've tried every other thing and it has failed; we're lying on the ground bleeding."

I kind of looked at the circuits and saw they've got regular structure. This output of this segment of the convolver should go to the input of that segment before, and on the chip, this is how it should be laid out, and these are the wires it should use, these are the registers it should use, and this should be a row, and this should be a triangle. So, I just developed Lava to let me have this composition by abutment as a concept, but also geometry: row, column, triangle, flip, etc. By doing that, I was able to get the customer design to fit onto the chip. It also looked beautiful; you could actually see the structure of the circuit that you drew on the whiteboard, you saw it on the actual chip, and it fitted, and it met timing (it was fast enough), and you could ship it to the customer. That's very satisfying. It's always great when you do research that is driven by trying to solve a problem, and it's a problem which no other current technique could solve. You're the last recourse to law, as it were, from the regular engineers who would otherwise rather write in Verilog.

It taught me the power of developing high-level abstractions for controlling low-level issues. The low-level issue is placement: what is next to where? How long is that wire between these two blocks? What will be its critical path? Because it's the slowest, longest wire that determines the speed of the overall system. You want to be productive in producing low-level systems, and you want to be agile because the first layout I produced didn't meet timing or it didn't fit. So, I had to experiment and try different decompositions, different layouts. But because I had this tool which gave me agility, I could try five or six things in one afternoon to find the one that worked. That was a real superpower. I could do that because I was productive; I had these high-level abstractions that made me productive, that let me explore more of a design space in an afternoon in a way that was just infeasible with VHDL or Verilog. I think that's quite exciting, and I'd like to see more of that in the world: more high-level abstractions that let you control low-level concerns.

**[00:31:12] SAT Solvers and Verification in Hardware**

**Aws:** Yeah, pretty sweet. And I wanted to ask about another paper of yours. While preparing for this, I found this paper called "Checking Safety Properties Using Induction and a SAT Solver" with Mary Sheeran and Gunnar Stålmarck. I was surprised because it seemed to me, when I saw it and from the date of the paper, that that was probably the first use of a SAT solver for verification. Is that correct?

**Satnam:** No, not at all. I think SAT solvers have been used for other kinds of verification. I'll give you one example from my own personal experience, because bounded model checking kind of started using SAT solvers.

**Aws:** Right, before bounded model checking.

**Satnam:** Yeah. So, up until a point, there was bounded model checking, and that was very much driven, I think, with a different representation, BDDs, not SAT solvers. But I think SAT solvers did enjoy use in other types of verification. One thing that's close to what I work on is logical equivalence checking, where you've got two digital circuits, where one is maybe a slight mutation of the other, and you want to ask, "Are they the same?" Logical equivalence checking (LEC) is a very common thing in the industry to work out if these two circuits are equivalent, and it can and typically uses SAT solvers. It still uses SAT solvers. The way that works is that you look at these circuit graphs, and you match up where the state elements are, which is obviously quite a heuristic thing; it might require some human intervention. Once you've matched the state elements in the two circuits, then you could just check using a SAT solver the equivalence between all the registers. If that passes, then you know the two circuits are the same. They're a standard part of the tool flow in a hardware design company. We run them all the time in our company, and all the main companies sell them.

So, I think that was one example of how SAT solvers were used for verification. What's perverse about that type of verification is this is a tool you buy from these hardware EDA tool vendors. The typical way you use it is to make sure that another tool you bought from the same vendor has not broken your circuit. So, you give your input to some tool that you paid millions of dollars for. The tool runs, it takes one circuit graph and produces another circuit graph, but you know that tool could have had a bug in it. How do you know it did the right thing? So, then you run your logical equivalence check to check for that particular instance: is the output circuit still having the same meaning as the input circuit? That's like a weird kind of checking; you're not checking that a human designer made an error, you're checking that that invocation of that tool didn't make an error.

But I think there are lots of great uses for that kind of stuff. One thing that's top of mind for me at the moment is machine learning compilers and the transformations they do, and possible mistakes that they might make. There, I think today it's a great opportunity for technologies like e-graphs (egg) to make sure that, for example, in the MLIR framework, the graph that comes into one stage of lowering and the graph that gets produced have the same semantics, and you haven't introduced an error. These things don't prove that your compiler is correct, but they can help you prove that for one invocation of your compiler, for a specific input, if there's a mistake, the compiler might be able to catch it.
So, yeah, there was some use of SAT, but I think that paper kind of led to a mini-revolution in being able to do these unbounded proofs. The algorithms in all the main model checking tools (JasperGold, etc.) – I run these tools occasionally, the YosysHQ tools – and occasionally we'll check some property, and it will say "proved by k-induction," and my heart misses a beat. It feels good.

**Aws:** That's cool. That's awesome.

**[00:35:17] Career Transitions: Academia to Industry (Xilinx, Microsoft Research, Google)**

**Aws:** So, should we shift to talking about career? Adrian, what do you think?

**Adrian:** Yeah, totally. So, Satnam, you are one of the academics in our community who has the benefit of having seen the industrial side from many different angles, as we were obviously just talking about Xilinx a while back. You've also been at Microsoft Research and at Google. One thing we're just curious about is what your sort of view on PL research looks like from that angle. In particular, how has your life as a sort of research-connected person changed when moving from company to company? Were things very different going from Microsoft to Google, for example? How did you find this? We'd love to know what your life has been like going through all these things.

**Satnam:** All these transitions were very, very different. I'll give a general answer, and then I'll describe a few specific transitions. I feel very fortunate to have done a kind of PL PhD in a fantastic group. I was in the functional programming group at University of Glasgow, started in '87, finished in '91. That was when Phil Wadler had come, Simon Peyton Jones, Mary Sheeran, John Hughes, John Launchbury, and an amazing collection of PhD students: Stephen Blott, Simon Marlow, Graham Hutton, etc. That was like a very vibrant, fantastic community to be in. I loved the collegiate atmosphere, all the talks and the presentations.

What it did was it gave me a toolkit. Being a PL PhD student gave me a toolkit for how to think about problems, how to decompose problems into smaller problems, how to solve them, how to communicate a problem to someone else, how to read and understand information and then represent it to someone else, how to get feedback and change my plans. All these skills that you learn as a PhD student, they're like an amazing toolkit for being a problem solver that have applicability way beyond programming languages. It gave me like a philosophical view of how to work and how to interact with people and how to solve problems. That was invariant. I worked in all these different places and I did quite different things, from designing hardware to working on Kubernetes to Android apps. I had this sensibility that was constant about how to think and work that helped me through all of these jobs.

Whenever I give a talk – I just gave one two days ago at Imperial College – I really emphasize to the PhD students, especially PL ones: don't have a blinkered view about you only working on PL or compilers or becoming a professor or whatever. You're being trained to be a problem solver; you're being trained to have this general toolkit to look at technical problems and solve them, and communicate with other people and participate in a wider group of people to do great creative things. At the time, I didn't understand this at all. It's only now when I stand back and look at it from a distance that I can see that that was the case. I feel we don't say that enough in our industry or community or universities, that that's the real thing that's happening when you're doing a PhD, becoming a postdoc, becoming a PhD researcher.

I started my life as an academic, and I had other academics as models. I tried to be like them and reproduce their success: wrote lots of papers, got grant funding, went to conferences, teaching, admin, etc. And that all went very well. But then I moved to Xilinx in California, and that was completely random. I was not planning to go; it was just something they offered me. So many of the transitions have been totally random. The randomness in this transition was as follows: my partner at the time, Susan Spence, was doing a PhD. She worked in persistent Java, in the same computer science department as me at University of Glasgow. Susan was offered an internship at Sun Microsystems, where she was collaborating with this team at Sun that was working on this persistent Java programming language. She was going to go for three months. I thought, "Three months away from Susan, how am I going to deal with that?" So, I contacted my Edinburgh friends that make these FPGAs (by that time, they'd been acquired by Xilinx). I went back to my friend John Gray. "John, I need a job in Silicon Valley for three months so I can be with Susan. Can you arrange it?" John pulls some strings, and I get to be an intern for three months at Xilinx. I go over, work for three months at Xilinx, make some Photoshop accelerator using their cards and a plugin for Photoshop. So, when you ran Photoshop, I did Gaussian Blur in FPGAs, whatever. Did my project, waved them goodbye on the way out, and they just put a job offer in my hand when I left. It kind of stopped me in my tracks because I hadn't thought about it. I thought I was lucky to get a permanent position after my PhD; I thought I was going to work at University of Glasgow forever.

I thought it was quite exciting working on that project which used my functional DSL in Haskell to do. I joined, making a real, solving a real problem, building an end-to-end system. There's a lot of satisfaction for me to just, even writing the C++ code for the plugin for Adobe Photoshop and so on, just doing the whole thing from there to hitting Gaussian blur, getting the image, having it transferred over PCI to the card's DRAM, having the FPGA process it, and getting results back and rendering the blurred result to a user. That was fantastic. What I love is when you do something end-to-end like that, you find out where all the dead bodies are buried. You find out where the real performance problems are. I kind of like that. I like being able to do something concretely end-to-end, where I think as an academic I didn't have that luxury. Most of my job was not to do that, and I didn't have resources to do that, and there maybe wasn't a lot of scientific value in doing it. So, I thought, "Okay, this is quite exciting, maybe I should do more of this end-to-end stuff, but I can't do it as a professor. I think I had to go to a company to do it."

So, I made the difficult decision to resign, which was very difficult because I had postdocs, I had PhD students, I had government-funded projects, commitments to other people. To walk away from all of that was heart-wrenching. Some of these people, I think, are still not talking to me. I joined Xilinx, and it was a big change. I was a junior academic, seven years in at that time. I worked very hard, I did not have a lot of spare time, and I didn't earn very much money. I was working hard to pay my mortgage. I arrived in Silicon Valley, and some people thought, "This is going to be really hard. You're in Silicon Valley, it's going to be super high stress, you'll be totally pressured." It was like a walk in the park. It was way less hard work and stress. I had free evenings. I had to work out how to use a free evening; I didn't know what to do with one before. I kind of thrived, I think, in this kind of Xilinx Labs environment where I developed Lava and then went on to do lots of other things. I love being at this intersection of solving really critical problems that people cared about, but bringing ideas and thinking from the world of PL and formal verification to bear down on that problem. I had a very good manager who gave me the time and space to do that. Whenever I've had any kind of success somewhere, it's always not just been me; it's been things like having a manager – in that case, it was Bill Carter, one of the people who founded Xilinx – who recognized the problem, the opportunity, and how to make me productive, then created an environment to make that happen. All my great managers have been like that, and all my terrible managers have tried to tell me what to do, and that never ends well.

**[00:43:14] Challenges in Industry: Management and Workplace Abuse**

**Aws:** So, speaking of terrible managers, you have a blog post about abuse and bullying in industry. This is taking a tangent here, but I'd love for you to tell us about it. I guess it resonated with many people.

**Satnam:** That's right. At one point in my career, I did – I mean, I'm very fortunate, most of my career I've had great managers, and that's largely why, if I've had any success, I owe a lot of it to their mentorship and their help. But occasionally, something goes wrong, you get a manager, and things don't work. I had, at one particular company, a particularly bad situation where I think I had a manager who I think was just maybe insecure, or I don't know why, but things didn't go well. I tried to go through HR to try and get this resolved, and I kind of learned a lot of lessons because I think up until that point, I was listening to the company mantra, which was, "We're Human Resources, we're here to help you, we're on your side, tell us your problems, we're your friends." Even this term "Human Resources" – I was amazed by this when I arrived in California in 1998. Someone said, "Oh, you'll have a meeting with Human Resources tomorrow." I said, "Human resource? Surely you're joking. What's the real name of the department?" But no, it really was called Human Resources.

So, I was tricked into thinking that they were on my side, but actually, they're not. They're on the side of the company. They're on the side of senior management, and when it comes to conflict, they're not your friends. I ended up leaving that particular company after quite a lot of acrimony and stress, and quite deep depression and anxiety on my part. I think that was really tragic because otherwise, I was doing really well at that company. I think I was contributing a lot; I enjoyed working with lots of people across the whole company. But this one clash was really... and I just learned a lot. I realized I was very naive about many aspects of how things like conflict are dealt with at companies, why do people behave badly and do the things that they do.

So, I tried to write that up in this blog article, a few bits of advice, and it just snowballed. So many people emailed me and told me their stories or told me they found the bits of advice I'd given useful. People DM me when they see I'm in their town; I meet them for coffee, and I chat to them about what's happening. People have really found that useful, and what it tells me is there's a market need for this. When I posted this on X and on LinkedIn, it was like a lightning rod; people really kind of jumped on it. Which makes me think, maybe in this community, we're not doing enough to honestly, seriously tackle abuse in work, abuse in the workplace. It's something we should talk more about, talk openly more about. I think another problem is this kind of LinkedIn culture where you just want to promote all the good things in your life, all the jobs that you got, all the successes you've had. You don't want to talk about your failures and things that are going wrong, or having to leave a job because you couldn't get on with your manager or whatever. The incentives are not set up like that. But maybe we'd be a bit of a healthier society if we were more honest about our vulnerabilities and what has worked for us and what has not worked for us.

**Aws:** Thanks for sharing this with us and with a lot of people.

**Adrian:** Indeed. You got me fantasizing about starting a startup that's like an inverted world version of LinkedIn where you're only allowed to post the hard things about life, only the failures and frustrations.

**Satnam:** I'd love to publish a resume which lists only the jobs I got rejected from, which will be way, way, way more impressive than what I actually got.

**[00:47:01] Current Work at Groq: AI Chips and Architecture**

**Aws:** Okay. So, on a somewhat positive note, you're now at Groq (Groq with a Q), the machine learning accelerator startup company. I was wondering if you could tell us what Groq is, what is the deal, and what attracted you.

**Satnam:** Groq makes silicon chips for machine learning inference. The chips have on them parallel systolic multipliers. That's their big hammer; they can do matrix multiply very, very quickly, very efficiently, way more efficiently than a CPU. A CPU doesn't have the computational intensity to hide memory access latency. So, it's a latency machine; it does one thing very quickly. If your job is you've got 17 totally different heterogeneous things to do and you want to do each one of those things as quickly as possible, a latency machine is what you want; a CPU does that. If what you want to do is something like matrix-matrix multiply, well, that's quite computation intensive. You need to get a lot of data right at your fingertips to get that matrix-matrix multiply going.

The dirty secret about GPUs is they work by oversubscription to give enough computational intensity to these tensor cores to keep you busy. So, while you're doing that, you overlap that with the next memory access to DDR or HBM, and it arrives. By the time you finish doing all those matrix multiplies, you've got your next one, and then you work on that, and then you hide memory access latency. That is why the leaders of Oracle and Twitter and so on are going to Nvidia, begging for GPUs.

We have a slightly different setup. Our chips, we've got the fine-grain parallel multipliers, but we've also got lots of SRAM on our chip: 88 independent banks of SRAM, all of which you can access in one clock cycle. So, phenomenal amounts of memory bandwidth. We have a deterministic architecture. When you compile a model onto a chip, you know on every clock cycle what instructions will execute. That's a mind-bending thing. When you compile a model, it produces an assembly file. The labels on the assembly instructions are the clock cycle that that instruction will execute in. Mind-bending, right? Almost nothing else has this property. This really helps us generate really efficient implementations of machine learning models. It helps us compose systems together to form big systems with predictable behavior and performance.

It's this collocation of this memory with the compute elements that gives us advantages because the key thing that people care about today are transformers. Transformers have decoders; for example, Llama 3 70B has 80 decoders which are all stacked next to each other. Inside the decoders, the computational element is attention. Attention involves multiplying some vectors, multiplying some big matrices. There's an optimization called the KV cache optimization which, in this case, turns these transformer computations from being matrix-matrix (which is memory-bound, so it suits GPUs, so you can be chugging away and have the computational intensity to mask memory access) to matrix-vector. Matrix-vector is compute-bound. Now the GPUs are not in a good situation anymore because they don't have the computational intensity to hide access to off-chip memory. But we have the KV cache there; it's in the SRAM. Nothing's going to go over a chip or a pin or a memory hierarchy to get to that data. So, our collocation of compute and data makes us a great match for compute-bound computations. That's why these things like Llama 3 and Gemma and Mistral really go fast on our chip. They have a very low latency, high throughput. That's the kind of special thing about our chip. We can't get the whole KV cache onto one of our chips, but what we do is we break the KV cache up across many, many, many chips and nodes and multiple racks of a system. But because of our determinism, we still get good composability, low latency, very good throughput. So, that's kind of the story of our company and why we're kind of doing well.

I worked in the design of the second chip, which I think is about to be announced. Sadly, it was in SystemVerilog; I couldn't get away with using Lava. And then I worked on its formal verification and some other aspects of it, and some aspects of the compiler as well. It's very exciting. Again, being able to work across the hardware-software boundary, I think is super exciting. It reminds me a little bit of the 6502 because we're back to a world where you know cycle by cycle what's going to happen, and you try and exploit that property for various kinds of optimizations.

**[00:52:15] Industry Attitudes Towards New Hardware Languages**

**Adrian:** It's been full circle for you. Since you mentioned it, Satnam, I can't help but ask. It makes total sense; a real chip company, obviously, SystemVerilog is going to be the go-to choice in the US at least. But how do you find the attitudes being in a chip company toward the idea of new hardware languages? Is everyone just fine with Verilog, everything's fine, or sort of striving?

**Satnam:** It's very, very hostile. I worked on two machine learning chips in a very different way. I worked on one at Google, and that was a project which from day one people decided, "We're going to use BlueSpec," which is this Haskell-variant hardware description language. They hired people with this deal: "This is BlueSpec." So, we only hired people that knew BlueSpec, or people we could repurpose as BlueSpec, or C++ people that we could brainwash into being BlueSpec people. That worked incredibly well. We were a very small team, and we built a working chip in a small amount of time. That was fabulous. It was very, very rapid, quick production of a working chip. I think we could do that again. I know of other people that are trying to do that again.

But in Groq, for whatever reason, the people who are in control of the hardware want to use SystemVerilog because hardware engineers are very suspicious of abstraction, and they are very, very risk-averse because it's a very risky business to make a chip. So, for example, even having records, record types, in interfacing ports. "Oh right, we can't have record types because it might cost us some silicon, or it might slow things down, or whatever." I don't know what's happening with these records. You're going, "No records, it's a compile-time abstraction. They have no cost whatsoever." But they still don't believe you. So, if you can't have a record type in the port of your SystemVerilog module, doing dependent types, polymorphic blah blah blah, effect handlers – really, they're going to call security and take your badge and throw you out of the building.

I think that's a shame. If I could do my own hardware company, I would force people to use a more civilized hardware description language, either BlueSpec or something, or a DSL that we make in some other language like Haskell or OCaml or something like that. And then I would hire the right engineers. I think one of the... I'm not sure that the best way to design a chip is to hire people who are normal chip designers who know SystemVerilog. I think the best way to make a chip is to hire the best PL people you know and retrain them how to do digital design. We did in the first chip company I was working at; we actually did quite a bit of that.

It reminds me of a trick a friend of mine played, Ian Pratt. He formed this company called XenSource. They went out to do hypervisors, and he needed, he was in Cambridge, and he needed the cleverest programmers he could find. This was his algorithm: he advertised in the Cambridge and London area for OCaml programmers. So, people came, and he interviewed OCaml code and stuff like that. Then when they got hired, they had to write this low-level C code. They're going, "I'm not a C programmer." He'd say, "You're very good at OCaml programming. I'm sure you could pick up C, and you'd be good at it." And because they were just brilliant programmers, they were just very creative, brilliant people. That was his bait-and-switch trick. The way to get good C programmers was to hire good OCaml programmers. So, I think there is something to be said for that.

**[00:55:41] Open Problems for the PL Community from Hardware Design**

**Aws:** So, flipping the question here, are there problems that you see in your chip design work where you're like, "Oh, I wish the PL community worked on this"? Maybe it's changed your views on what languages should look like, or other tools like static analysis?

**Satnam:** I do actually. When you do a real chip, you have at every level of abstraction, you have a different view of the hierarchical decomposition of the chip. When you are at the highest level, you do logic design, you have the hierarchical view of the top-level block, the sub-blocks, the other blocks, and how they're all connected and wired. If you're a software engineer, that hierarchical decomposition persists all the way to when the assembly code is generated, modulo cross-boundary code optimization, cross-module code optimization. Chips are not like that at all because in the real chip, you also have power wires that distribute power and ground, wires that distribute a clock (multiple clocks), wires that distribute enable signals, and so on.

So, when it comes to doing the physical design, you have a quite different hierarchical decomposition which cares about how these global wires are fed through a chip and how things are laid out physically on the chip and organized. That means the RTL code that the logical design engineers write, they have this illusion of what the hierarchy is. The actual hierarchy and organization of what gets turned into gates is very, very different from that. The design process is sequential. You have to do logic design and freeze it. Then you do these other verification or physical design things. Then if you want to go back and make a change, it's very hard to do because probably somebody manually, through some tools, mutated and reorganized your design. You've kind of lost the link from the original code. It'd be great if we could have programming language support for describing different types of hierarchical decomposition and keeping them in check, in sync, so you can then go forward and backwards between them. It's one program, it's one circuit, but you want to have different views of it. I think that'd be cool. I don't know anything that does that.

**Adrian:** That would be incredibly cool. I can't help but note it sounds really hard to do in the current landscape of hardware tools that are closed source and extremely expensive. Something's got to shift to sort of let academics get access to the levels of abstraction they need.

**Satnam:** I'm a big fan of the people that do Verilator, the people that do YosysHQ. The formal verification work that we do at our company uses the YosysHQ tools, which are then based on Verilator and Z3 and CVC. Because we can't afford JasperGold, the heavyweight industry standard tool. But I'm quite impressed by what they do, and I think if that ecosystem could get more traction, more support from other companies or universities, I think there's great stuff that could happen. There are various research groups around the world doing great things, like yours. Great stuff happening, I think, at Cambridge in Tobias Grosser's group, like Lean MLIR – two worlds that were previously separate in my mind, clashing together. So, if there's someone to coordinate all these things into a wider master vision of how the world of chip design could be different, I think there really is opportunity and need there because nobody really likes what they have at the moment. The real hardware engineers refuse to use any of these other tools but also hate the tools that they have.

**Adrian:** Yeah, it's kind of bleak when you put it that way, I guess.

**[00:59:37] Reflections on Career Shifts and Learning to Code Professionally**

**Satnam:** Another strange transition I made: the transition from University of Glasgow to Xilinx changed one axis – I went from academia to industry – but it kept another axis constant because I joined the new Xilinx Labs. So, I still wrote papers, I went to conferences, I was on program committees. I still kind of behaved a bit like a researcher, an applied researcher.

But the most stark change I made was when I left Microsoft Research Cambridge, where I very much behaved like a researcher. My job was to be Satnam, which was a very nice job to have; I think I'm good at that. To joining Google in Silicon Valley, where I was just a software engineer and I had to write code for distributed systems. It was a big shock. I learned then that I could not code, even though I've been coding my entire life and I'd written, I don't know, millions of lines of code or whatever. I'd never had to write code as a professional software engineer for a production project with deadlines and so on. So, the whole process of code review and everything, that was all kind of new for me. In my research projects, I never had to do that.

I had this fantastic tech lead, Daniel Spoonhower, a PL person who did his PhD with Bob Harper at CMU. And my peers were all over half my age, and I learned from them; they taught me how to become a professional software engineer. It was a very humiliating process, but I was very thankful and grateful for it. I think I went on to be a pretty decent Google engineer and ended up working, being one of the early people in Kubernetes, which is bizarrely the most successful thing I've done in my entire career and utterly unrepresentative of everything else I've done. So, that was a difficult journey, the most stark, because all the bits kind of flipped. I went from a research world to production DevOps world with a pager. That was a difficult transition.

**Aws:** You had moments where you were like, "Oh my god, what have I done?"

**Satnam:** Yeah, absolutely. Before, when I decided what projects I was going to do, I took my time. I could write in Haskell or F# or whatever. I was in control of my destiny, and I wrote my own code. I didn't have to have my code reviewed. I just thought, "It's like research code at MSR; I could just check it in." To getting four lines of code checked in could take a week, and I thought, "What have I done here?" But I stuck with it, and it was great. I think I learned a lot. Eventually, I loved being at Google. I mean, I don't work at Google anymore; I've worked for Google twice so far. I enjoyed being a Google engineer, and I identify, even though I don't work at Google, with being a Google engineer, with other Google engineers. It was a very useful experience and taught me much about software and how to write software in a social manner, in a way that's kind of durable, that could be evolved, that's reliable, that's scalable – things I never really had to do before. Those turned out to be very useful life skills. I'm 58, man, I'm still writing code and designing circuits every day for a living. I think if I had not been through that process, I'd probably just be a VP drawing boxes and arrows, writing documents, and that's the only work I'd do.

**[01:03:09] Maintaining Ties with Academia**

**Aws:** So, we wanted to ask, since you've been in industry for a while but you've kept ties with academia – I see in your CV, every once in a while, you do a visiting lecture gig or teach a course, something like that. What attracts you to doing that? It's way more work than having one job.

**Satnam:** I always love teaching. Remember at the beginning of this podcast, I talked about how inspired I felt by my professors and how they taught, how amazing that was. I've always wanted to be like that as well. So, I've always enjoyed teaching, trying to explain things, giving examples, writing demos of code, and running them. I always try to do a bit of that.

When I worked for Microsoft in Redmond, when I lived in Seattle, I also was lucky to have an affiliate position at the University of Washington. In the electrical engineering department, I taught a course on how to test VLSI chips, and that was fabulous. I loved doing that. When I was at MSR Cambridge, I actually had a 20% appointment as a professor at University of Birmingham, gave some lectures there, and had a nice research group to hang out with. I was a visiting lecturer at Imperial College; I'd rock up every now and then and talk to PhD students. I love that. I especially love just talking to PhD students and postdocs and giving just a different perspective on their project and their direction from their adviser. I think they learn a little bit, I learn a little bit from them about what they're doing.

Here in the Bay Area, South Bay area, I was lucky to be connected up with the Lindsey Kuper crowd and teach a course in Verilog, which was then at UC Santa Cruz. I taught that course during the pandemic, so my screen was a grid of black rectangles, and the University of California says I'm not allowed to make the students turn their cameras on. That was a bit of a dispiriting experience. When I was offered to do it again, I declined. It made me realize how much I love being in the room with the students and doing teaching as a kind of a two-way thing, and I didn't like teaching to an array of black boxes.

It just gives me lots of energy. I think it's one of the sources of creative energy for me, going to these places, talking to the students, giving the talks, getting feedback, learning what other people are thinking and working on, and what excites them. I really miss the collegiate atmosphere of a university department. That's the thing I miss the most from University of Glasgow: all the other research groups, banging into someone in the corridor and having a chat about some related thing, all the great invited speakers coming in. I think that's a fabulous, very special thing, and I've never really experienced that even in research labs. I never really experienced that in industry. That's one thing I'm very jealous about for my academic friends. So, I try, whenever I can, to find an excuse to turn up at a university and give a talk and hang out with some cool people.

**[01:06:00] Time Management and Integrating Academic Activities**

**Aws:** Can you tell us your time management tips? How do you manage to do both of those?

**Satnam:** Well, I don't do very much of these things. The Verilog course I taught, that was just one course that I taught; I taught half of it, somebody else taught the other half. That was a giant amount of time commitment. For visiting other people, I always collocate that with some trip I'm doing anyway. If I'm going to a conference, giving a talk, then I'll tack that on. On Monday and Tuesday, I gave a talk at a new computer architecture conference in London called REACH 2024. So, I was in London, so I extended my visit by one day and I went to Imperial College. I spoke to Ali Donaldson saying, "Ali, do you want me to turn up and give a talk?" And Ali said okay. I gave a talk there. Before I went to REACH, I stopped in Chicago and gave a talk at Purdue. So, I try to tack them on to other trips that I'm doing anyway. It definitely takes up a chunk of my time, and my productivity at work is definitely lower as a result of it, but I value it enough. I think I get enough out of it, and it gives me enough kind of creative energy and inspiration for other things that I might tackle at work, that I feel like it's worth doing.

**[01:07:24] Existential Questions in PL and the Beauty of Programs**

**Aws:** So, you mentioned at the beginning that for a while you were a physics student, and physics has a lot of kind of open existential questions, right? Like the theory of everything and the standard model and what have you. Is there such a thing in PL, and if so, what is it? Is that what attracted you to PL? Was there some existential question?

**Satnam:** I'm not sure I thought about it in that way, but I think there definitely is something about language: having a language to talk about the world, and having a language to model the world. Then when you have that model, it's interactive; you can do things, you can do experiments with that. I think it goes back to what I said at the beginning about this wonderful synergy between functions, recursive functions, lazy evaluation, and how digital circuits kind of work. That just seems like a magical thing to me.

Some people use programming languages because they have to get their jobs done. When I talk to my friends who are systems researchers, they'll write in C or C++, and it's just to get the job done because they have to run an experiment and get a graph and a chart. A program is just like something you have to do, some side effect of getting your job done. I think programs are beautiful. I view them in the same spectrum as literature and poetry or whatever because they exist there to help us communicate ideas from one person's head to another person's head, from a person's head to a machine, to maybe another person's head or whatever. I kind of do – I'm not sure I entirely believe it – but there's a little bit of truth and beauty that I really believe in. So, I feel if you can have the right abstractions and mechanisms in your programs to then model something in the world, in some way you've kind of captured something kind of truthful, right? A universal language where everyone can understand it, agree to the semantics of it. I just love programs for how they are, for their beauty. I don't even care if you run them; they just look so beautiful. That's what I kind of love about Haskell and these kind of lovely types. It's the aesthetics of the programs; I value the programs for themselves.

**[01:09:39] FP Castle**

**Aws:** And I guess because of that, you want to build an FP Castle. So, maybe tell our audience about this idea.

**Satnam:** I have this dream. Hopefully, Groq will make me fabulously wealthy, and I'll have enough money to then buy a castle in Scotland. What I want to do is something a little bit like Dagstuhl, but the scope is like a year or longer. I want to invite people to this remote castle to come and hang out, do functional programming, do C++, do systems, be a jazz pianist, whatever, have some paintings, whatever. Where people are just in a remote part of Scotland with beautiful scenery, an inspiring background. You don't necessarily need to be in computer science, but people are there for an extended period of time, interacting with each other, learning from each other, being inspired by each other. It could be through poetry, could be through music, could be through coding. I plan to do a lot of cooking. We'll have a whiskey cellar and cook lots of nice meals. I think that'll be quite nice to have a kind of a jamming session like that, a bohemian jamming session lasting about a year or so. That's FP Castle. So, you can go to fpcastle.com and sign up. You could just create a PR in GitHub and send it to me.

**Aws:** Oh, nice. Okay. I'm going to sign up.

**Adrian:** Yeah. Everyone listening should go to fpcastle.com and sign up.

**Satnam:** The rooms are running out!

**[01:11:17] Food, Cooking, and Restaurants**

**Aws:** Cool. So, I guess, should we talk about food? You're well known to be a foodie. Do you go with that term?

**Satnam:** I'm happy to say foodie. I believe even my cookbooks... None of these, you might think they're computing books, but every single one of those is a cookbook. So, I love food, I love cooking. I think it's just a wonderful way to interact with other people. I got into cooking through someone I met at university who also used cooking as a social vehicle. For relaxation, everyone seeks some kind of task that they could be totally absorbed in so they can relax from work and detach. For me, it's like cooking: trying to cook a nice meal and inviting friends around and having nice food and nice wine over a nice evening. So, I do a lot of that. I cook many different parts. I have a cooking log; the cooking log goes back decades, it's now a digital document. I record who came for dinner, what I cooked, what worked, what didn't work, who got on with whom, who didn't get on with whom. There's a separate secret document which is the incompatibility matrix, which shows which guests are incompatible with which other guests. So, I take it kind of seriously. It's one of the main ways I interact and socialize with other people. When I go to conferences, I try and get people together and go out to nice dinners because it's a nice informal way to get to know other people and interact and socialize with them. So, yeah, it's become a big part of my life and identity, especially my identity in social media.

**Aws:** So, what is your favorite restaurant?

**Satnam:** My favorite restaurant is in Glasgow. My daughter, she's a final year computer science student at the University of Strathclyde, taught by many of my friends. Downstairs from her is a restaurant called Brett, which does just fabulous Scottish-inspired food. It doesn't have a Michelin star or anything like that, but it's just really great produce, wonderful scallops. They cook beetroot amazingly. They'll do fantastic things with mushrooms, and they have a great organic wine selection. So, it's just an informal setting, gorgeous food. It happens to be just downstairs from my daughter's apartment, or maybe my daughter's apartment just happens to be upstairs from it. So, I love that place a lot. If you go to Scotland or you go to Glasgow, I really recommend going to Brett. I mean, I can list plenty of Michelin-starred restaurants I went to where I've had wonderful meals, and they're all fantastic as well. But I think it's always great when you can find something that doesn't have stars but is still really good and a bit more informal because I don't need all the posh aspects that go with posh restaurants.

**[01:14:06] Favorite Cookbook and Scotch**

**Adrian:** How about a favorite cookbook? Got one?

**Satnam:** Well, I have it here. It's called "Dabbous," and it's a cookbook of a restaurant called Dabbous that I ate at on Wednesday night in London with my daughter. It's my most used cookbook. You can see it's like falling apart. If you come for a fancy dinner at my house, there's a good chance I will cook out of this book for you. I love it. A lot of books have kind of fairly straightforward recipes. This one has a warning at the front saying, "Make sure that you really are the kind of person that's up for cooking the recipes in this book because a lot of them are quite technical and complicated, take many, many days to do." I've cooked many recipes out of there. It's always interesting; when I ate at the restaurant on Wednesday, I was getting things somewhat similar to what's in the book. So, it's always interesting to see, did I get it right or how different was it from the standard version. I think that's fabulous. It's a fabulous cookbook. I've been cooking from it for a decade almost.

**Aws:** And since you're Scottish, what's your favorite Scotch?

**Satnam:** My favorite Scotch is this one called Octomore. [Shows bottle] This bottle sadly is empty, but it's a very, very special bottle. Many people who drink whiskey or are into whiskey will know that there's a brand of whiskey called Octomore, and you can buy an Octomore today, but the Octomores you get today have really kind of diverged. This is the first batch of Octomore that was ever made. This bottle was given to me by a very good friend of mine, Ralph, and he bought futures in the first batch of Octomore. He bought 12 bottles and he gave me one. This comes from a distillery called Bruichladdich on Islay, which is known for its smoky whiskies like Laphroaig, Lagavulin. What happened was someone bought this distillery, and they looked at the stock, some of which was terrible, some okay. They tried to salvage some whiskies and blend with other whiskies to try other barrels to kind of reboot the whiskey and rebrand it. Octomore was one of the first things of that project that they did, and it was just absolutely fabulous. I remember my first sip; I remember where I was sitting, where I had it. It was absolutely mind-blowingly excellent. It was a very smoky, peaty whiskey, but just absolutely exquisite in its taste. It's gone now. I don't think you can get any more of that V1 Octomore. Now we're on to V... I can't remember where it is. The current Octomores are very, very astringent, very, very smoky, you know, 240 parts per million, still very, very good, but I don't think they have the kind of delicacy and balance of this original Octomore. This was, I think, the best whiskey I've ever had, sadly gone.

**Adrian:** The current Octomore seems to be 14.2, for what it's worth.

**Satnam:** You're right.

**[01:17:04] Advice for Younger Researchers**

**Adrian:** So, beyond those recommendations, thank you by the way, I'll be checking out all these things. We have again saved our most difficult question for last, which is: what advice do you have for younger researchers than you? To keep it concrete, let's say to graduate students and to undergraduates, what advice would you give to them?

**Satnam:** I'll just do a little bit of a repeat of what I said earlier, which is to have a broader view of what you are doing. When you do a PhD, things get very narrow and very laser-focused, and often you can kind of think, "Well, why am I doing this esoteric thing with this weird type system or whatever? What good is this? Will anyone ever care?" But what's really happening is you're learning how to think. You're learning how to read, assimilate information, categorize information, compare one thing to another, do an evaluation and judgment, then express yourself by trying to write a paper or evaluation or give a talk to someone else, and then try to collaborate with someone else to try and solve this bit of the problem. Although you're doing these things for a particular thing, you're really sharpening these tools you have to be a general problem solver. You're just developing your toolkit for thought and problem solving, which has wide applicability across probably all of computer science and beyond computers, probably in your personal life and many other things that you have to do in the world as well.

So, I would say kind of hang on to that because I know from my own personal experience, when you're really deep down in a very deep, esoteric thing, it could be dispiriting. You think, "Why am I doing this? Why doesn't anyone actually care?" Think about the second and third-order effects of what's actually happening. I would set myself as an example of that, right? I did all this kind of esoteric stuff with Miranda and everything, non-standard interpretations and lattices and things like that, but yet it proved useful in quite diverse areas of computer science.

**[01:19:16] Conclusion**

**Adrian:** Wonderful. Well, thank you so much for taking the time.

**Aws:** Yeah, thank you Satnam. This was really wonderful. I really appreciate it.

**Satnam:** Thank you very much for inviting me. It was good fun.
